{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "87cd2d3f",
      "metadata": {
        "id": "87cd2d3f"
      },
      "source": [
        "# Sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91cefec1",
      "metadata": {
        "id": "91cefec1"
      },
      "source": [
        "import some relevant modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "8f33945d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f33945d",
        "outputId": "3c29c107-3616-4c19-d5cc-bc120b232d56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "# import libraries\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from tensorflow import keras\n",
        "import re\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "#importing the metrics from sklearn\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score, precision_score, recall_score, f1_score\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "id": "4df55a41",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "4df55a41",
        "outputId": "07eea640-8011-4eb7-d2b1-f6b0d6e41a78"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>This quiet , introspective and entertaining in...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Even fans of Ismail Merchant 's work , I suspe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>A positively thrilling combination of ethnogra...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Aggressive self-glorification and a manipulati...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   SentenceId                                             Phrase  Sentiment\n",
              "0           1  A series of escapades demonstrating the adage ...          1\n",
              "1           2  This quiet , introspective and entertaining in...          4\n",
              "2           3  Even fans of Ismail Merchant 's work , I suspe...          1\n",
              "3           4  A positively thrilling combination of ethnogra...          3\n",
              "4           5  Aggressive self-glorification and a manipulati...          1"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ],
      "source": [
        "# read the files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "train_set = pd.read_csv(\"/train.tsv\", sep = '\\t')\n",
        "dev_set = pd.read_csv(\"/dev.tsv\",sep ='\\t' )\n",
        "test_set = pd.read_csv(\"/test.tsv\",sep ='\\t')\n",
        "train_set.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "ca6d4f2f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca6d4f2f",
        "outputId": "56a3e7e3-c217-4f59-a949-7b028d982826"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    A series of escapades demonstrating the adage ...\n",
              "1    This quiet , introspective and entertaining in...\n",
              "2    Even fans of Ismail Merchant 's work , I suspe...\n",
              "3    A positively thrilling combination of ethnogra...\n",
              "4    Aggressive self-glorification and a manipulati...\n",
              "5    A comedy-drama of nearly epic proportions root...\n",
              "6    Narratively , Trouble Every Day is a plodding ...\n",
              "7    The Importance of Being Earnest , so thick wit...\n",
              "8                But it does n't leave you with much .\n",
              "9              You could hate it for the same reason .\n",
              "Name: Phrase, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ],
      "source": [
        "# reading the phrase\n",
        "phrase = train_set['Phrase']\n",
        "phrase.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "857e0fa5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "857e0fa5",
        "outputId": "18c8e497-217c-4b6f-ca60-3c1ab1bfbb52"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f7498162910>"
            ]
          },
          "metadata": {},
          "execution_count": 125
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# reading the sentiment\n",
        "sentiment = train_set[\"Sentiment\"]\n",
        "sentiment.head(10)\n",
        "sentiment.hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5fca0ac",
      "metadata": {
        "id": "e5fca0ac"
      },
      "source": [
        "# preprocess text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "c3b89e90",
      "metadata": {
        "id": "c3b89e90"
      },
      "outputs": [],
      "source": [
        "# define preprocessDataset\n",
        "def preprocessDataset(text): \n",
        "        \n",
        "    text = str(text)\n",
        "    \n",
        "    #remove single quotes \n",
        "    text = text.replace(\"'\", \"\")\n",
        "    \n",
        "    \n",
        "    #word tokenization using text-to-word-sequence\n",
        "    tokenized_train_set = text_to_word_sequence(text,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',split=\" \")\n",
        "\n",
        "\n",
        "    #stop word removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stopwordremove = [i for i in tokenized_train_set if not i in stop_words]\n",
        "    #print (stop_words)\n",
        "     \n",
        "    #join words into sentence\n",
        "    stopwordremove_text = ' '.join(stopwordremove)\n",
        "    #print(stopwordremove_text)\n",
        "        \n",
        "    #remove numbers\n",
        "    numberremove_text = ''.join(c for c in stopwordremove_text if not c.isdigit())\n",
        "    #print(output)\n",
        "        \n",
        "    #Stemming\n",
        "    stemmer= PorterStemmer()\n",
        "\n",
        "    stem_input=nltk.word_tokenize(numberremove_text)\n",
        "    stem_text=' '.join([stemmer.stem(word) for word in stem_input])\n",
        "    #print(stem_text)\n",
        "    \n",
        "    #lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def get_wordnet_pos(word):\n",
        "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "        tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "        return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "    lem_input = nltk.word_tokenize(stem_text)\n",
        "    lem_text= ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in lem_input])\n",
        "    #print(lem_text)\n",
        "    return lem_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the train set"
      ],
      "metadata": {
        "id": "boqKgPsgNfhQ"
      },
      "id": "boqKgPsgNfhQ"
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "id": "b5a7c854",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5a7c854",
        "outputId": "4a9ecdb9-9cf8-43cc-f5d6-65bad33a16d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    seri escapad demonstr adag good goo also good ...\n",
              "1       quiet introspect entertain independ worth seek\n",
              "2    even fan ismail merchant work suspect would ha...\n",
              "3    posit thrill combin ethnographi intrigu betray...\n",
              "4                aggress self glorif manipul whitewash\n",
              "Name: Phrase, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ],
      "source": [
        "# apply preprocessing in the train set\n",
        "train_set['Phrase'] = train_set['Phrase'].apply(preprocessDataset)\n",
        "phrase = train_set['Phrase']\n",
        "sentiment = train_set['Sentiment']\n",
        "phrase.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the dev set"
      ],
      "metadata": {
        "id": "6Kaw0h0_NjRO"
      },
      "id": "6Kaw0h0_NjRO"
    },
    {
      "cell_type": "code",
      "source": [
        "# apply preprocessing in the dev set\n",
        "dev_set['Phrase'] = dev_set['Phrase'].apply(preprocessDataset)\n",
        "dev_phrase = dev_set['Phrase']\n",
        "dev_sentiment = dev_set['Sentiment']\n",
        "dev_phrase.head()\n"
      ],
      "metadata": {
        "id": "DsSV3cpAsxBz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9577883-2694-4103-c805-98fdb1b1152b"
      },
      "id": "DsSV3cpAsxBz",
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                             scorcher\n",
              "1    spectacular everi sen word even know orc uruk hai\n",
              "2    slacker jokey approach colleg educ disappointi...\n",
              "3    ok movi someth sitcom apparatu line work humor...\n",
              "4                        although idea new result tire\n",
              "Name: Phrase, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the phrase of test set"
      ],
      "metadata": {
        "id": "KTfSDSyWNzcM"
      },
      "id": "KTfSDSyWNzcM"
    },
    {
      "cell_type": "code",
      "source": [
        "# applying preprocessing in the test set\n",
        "test_set['Phrase'] = test_set['Phrase'].apply(preprocessDataset)\n",
        "test_phrase =test_set['Phrase']\n",
        "test_set.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "H52UgiX8w-go",
        "outputId": "9a3bfdfa-291d-43cc-f2a4-c51d48f58d36"
      },
      "id": "H52UgiX8w-go",
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8545</td>\n",
              "      <td>intermitt plea mostli routin effort</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8546</td>\n",
              "      <td>kidman realli thing worth watch birthday girl ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8547</td>\n",
              "      <td>get rhythm movi becom headi experi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8548</td>\n",
              "      <td>kept wish watch documentari wartim navajo acco...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8549</td>\n",
              "      <td>kinnear nt aim sympathi rather deliv perform s...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   SentenceId                                             Phrase\n",
              "0        8545                intermitt plea mostli routin effort\n",
              "1        8546  kidman realli thing worth watch birthday girl ...\n",
              "2        8547                 get rhythm movi becom headi experi\n",
              "3        8548  kept wish watch documentari wartim navajo acco...\n",
              "4        8549  kinnear nt aim sympathi rather deliv perform s..."
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec35e6e6",
      "metadata": {
        "id": "ec35e6e6"
      },
      "source": [
        "# Naive Byes "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "id": "a0ae156c",
      "metadata": {
        "id": "a0ae156c"
      },
      "outputs": [],
      "source": [
        "# define word collection\n",
        "def wordCollection(phrase, sentiment):\n",
        "    words = []\n",
        "    for i in phrase[phrase['Sentiment'] == sentiment]['Phrase'].str.split():\n",
        "        for j in i:\n",
        "            words.append(j)\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "e2679c14",
      "metadata": {
        "id": "e2679c14"
      },
      "outputs": [],
      "source": [
        "negative = wordCollection(train_set,0)\n",
        "somewhat_negative = wordCollection(train_set,1)\n",
        "neutral = wordCollection(train_set,2)\n",
        "somewhat_positive = wordCollection(train_set,3)\n",
        "positive = wordCollection(train_set,4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "id": "50b6975f",
      "metadata": {
        "id": "50b6975f"
      },
      "outputs": [],
      "source": [
        "# define map_3_value\n",
        "def map_3_value(sentiments):\n",
        "    value_scale = {\n",
        "        0: 0,\n",
        "        1: 0,\n",
        "        2: 1,\n",
        "        3: 2,\n",
        "        4: 2,\n",
        "    }\n",
        "    \n",
        "    return np.array([value_scale[sentiment] for sentiment in sentiments])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mapping the 3 value sentimental scale in dev and train"
      ],
      "metadata": {
        "id": "DUtSJTtLOLPG"
      },
      "id": "DUtSJTtLOLPG"
    },
    {
      "cell_type": "code",
      "source": [
        "# mapping 3 value sentimental scale\n",
        "dev_sentiments_3 = map_3_value(dev_sentiment)\n",
        "train_sentiments_3 = map_3_value(sentiment)"
      ],
      "metadata": {
        "id": "7O6UsptTyEUm"
      },
      "id": "7O6UsptTyEUm",
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing it\n",
        "print(list(sentiment[:20]))\n",
        "print(list(train_sentiments_3[:20]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An1GPL6_yhRn",
        "outputId": "82c5afad-3f58-40b2-c6bb-8383ddf5b825"
      },
      "id": "An1GPL6_yhRn",
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, 1, 3, 1, 4, 1, 3, 1, 1, 1, 1, 4, 3, 3, 3, 3, 2, 1, 2]\n",
            "[0, 2, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 1, 0, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "id": "a19d68d6",
      "metadata": {
        "id": "a19d68d6"
      },
      "outputs": [],
      "source": [
        "# def calculate_prior(sentiment,Y):\n",
        "#     classes = sorted(list)\n",
        "#     prior = []\n",
        "#     for i in classes:\n",
        "#         prior.append(len(sentiment(sentiment[Y])==i)/len(df))\n",
        "#     return prior"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting unigrams from the word list of all the sets"
      ],
      "metadata": {
        "id": "TPOQmICqOad7"
      },
      "id": "TPOQmICqOad7"
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "id": "0bec25a4",
      "metadata": {
        "id": "0bec25a4"
      },
      "outputs": [],
      "source": [
        "# extract unigrams from the word list\n",
        "def extract_unigrams(phrase):\n",
        "  return [\n",
        "    #checking for the words while extracting unigrams\n",
        "    word.lower() for word in re.findall(r'\\b[A-Za-z]{2,}\\b', phrase)\n",
        "  ]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vocabulary list from extracted unigrams"
      ],
      "metadata": {
        "id": "SdVvZNVJOfkB"
      },
      "id": "SdVvZNVJOfkB"
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "id": "9ea036af",
      "metadata": {
        "id": "9ea036af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19950b70-bf74-46a7-fc29-a3cb47e49df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['thandi', 'midway', 'hayse', 'bentley', 'palestinian', 'mermaid', 'oft', 'splatter', 'hepburn', 'downright', 'grandli', 'dollar', 'libidin', 'pool', 'smother', 'exactli', 'ricochet', 'scorces', 'essenc', 'molina', 'clone', 'booth', 'war', 'territori', 'progress', 'ravag', 'immun', 'pile', 'intellig', 'textur', 'seren', 'saddest', 'atyp', 'ape', 'stomach', 'drumbeat', 'campi', 'publicist', 'excus', 'revers', 'sandler', 'tact', 'horn', 'diamond', 'sprawl', 'cast', 'although', 'europ', 'guess', 'paulin']\n"
          ]
        }
      ],
      "source": [
        "# printing the list of vocab\n",
        "vocab_list = { unigram for phrase_word in phrase for unigram in extract_unigrams(phrase_word) }\n",
        "print(list(vocab_list)[:50])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating dictionary for word to vocabulary id"
      ],
      "metadata": {
        "id": "sreSxqtrOl_n"
      },
      "id": "sreSxqtrOl_n"
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "id": "794dba63",
      "metadata": {
        "id": "794dba63"
      },
      "outputs": [],
      "source": [
        "# vocab id to word dict\n",
        "vocab_id_to_word_dict = dict(enumerate(vocab_list))\n",
        "word_to_vocab_id_dict = {v: k for k, v in vocab_id_to_word_dict.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating unigrams for dev, train and test sets"
      ],
      "metadata": {
        "id": "LKoUluvIOrhh"
      },
      "id": "LKoUluvIOrhh"
    },
    {
      "cell_type": "code",
      "source": [
        "# creating unigrams for the datsets\n",
        "dev_unigrams = [extract_unigrams(phrase) for phrase in dev_phrase]\n",
        "train_unigrams = [extract_unigrams(phrase) for phrase in phrase]\n",
        "test_unigrams = [extract_unigrams(phrase) for phrase in test_phrase]"
      ],
      "metadata": {
        "id": "Kd8Q1EmrFvFQ"
      },
      "id": "Kd8Q1EmrFvFQ",
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorise the unigrams"
      ],
      "metadata": {
        "id": "55lWkzJGOynv"
      },
      "id": "55lWkzJGOynv"
    },
    {
      "cell_type": "code",
      "source": [
        "# define vectorise_unigram\n",
        "def vectorise_unigram(data_unigrams, vocab):\n",
        "    vec = []\n",
        "    for unigram in data_unigrams:\n",
        "        counter_unigram = Counter(unigram)\n",
        "        vec.append([counter_unigram[v] for v in vocab])\n",
        "    \n",
        "    return np.array(vec)"
      ],
      "metadata": {
        "id": "ok2GxJCOGDt1"
      },
      "id": "ok2GxJCOGDt1",
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorising all the sets"
      ],
      "metadata": {
        "id": "_RY1ANucO5kb"
      },
      "id": "_RY1ANucO5kb"
    },
    {
      "cell_type": "code",
      "source": [
        "# building up dev , train and test vec\n",
        "dev_vec = vectorise_unigram(dev_unigrams, vocab_list)\n",
        "train_vec = vectorise_unigram(train_unigrams, vocab_list)\n",
        "test_vec = vectorise_unigram(test_unigrams, vocab_list)"
      ],
      "metadata": {
        "id": "DTvdI7_uGVRT"
      },
      "id": "DTvdI7_uGVRT",
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prior and Word Probability calculation"
      ],
      "metadata": {
        "id": "Tp45YcVJO9rL"
      },
      "id": "Tp45YcVJO9rL"
    },
    {
      "cell_type": "code",
      "source": [
        "# define function to calculate prior probability for sentiment\n",
        "def calculate_prior_probability_for_sentiment(sentiments):    \n",
        "    counter_sentiment = Counter(sentiments)\n",
        "    return np.array([v for (_, v) in sorted(counter_sentiment.items())]) / len(sentiments)"
      ],
      "metadata": {
        "id": "7yb9sgo5HDou"
      },
      "id": "7yb9sgo5HDou",
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define function to calculate word probability for sentiment\n",
        "def calculate_word_probability_for_sentiment(total_vocab, data_unigrams, sentiments):\n",
        "    probs = np.zeros((total_vocab, len(set(sentiments))))\n",
        "    for i, unigram in enumerate(data_unigrams):\n",
        "        for word, count in Counter(unigram).items():\n",
        "            probs[word_to_vocab_id_dict[word]][sentiments[i]] += count\n",
        "    # With Laplace smoothing for prob check\n",
        "    return np.log10(probs[:] + 1) / (probs.sum(axis=0) + total_vocab)"
      ],
      "metadata": {
        "id": "aODj03dIHZjT"
      },
      "id": "aODj03dIHZjT",
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prior and Word Probability for training sentimental 3-value scale"
      ],
      "metadata": {
        "id": "LLfdiivRPD6f"
      },
      "id": "LLfdiivRPD6f"
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate prior and word prob for training set for 3-value scale\n",
        "train_prob_3 = calculate_prior_probability_for_sentiment(train_sentiments_3)\n",
        "word_3_prob = calculate_word_probability_for_sentiment(len(vocab), train_unigrams, train_sentiments_3)\n",
        "\n",
        "print(train_prob_3, '\\n')\n",
        "print(word_3_prob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUV3-VlAHuYp",
        "outputId": "923b5b7f-dc5d-4b33-fd92-014ed3dcb6b0"
      },
      "id": "KUV3-VlAHuYp",
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.38331784 0.19577633 0.42090583] \n",
            "\n",
            "[[0.00000000e+00 1.30434592e-05 0.00000000e+00]\n",
            " [8.06964389e-06 0.00000000e+00 7.31952235e-06]\n",
            " [0.00000000e+00 1.30434592e-05 0.00000000e+00]\n",
            " ...\n",
            " [0.00000000e+00 0.00000000e+00 7.31952235e-06]\n",
            " [0.00000000e+00 0.00000000e+00 1.16011684e-05]\n",
            " [0.00000000e+00 0.00000000e+00 7.31952235e-06]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate prior and word probability for training set for 5-value scale\n",
        "train_prob_5 = calculate_prior_probability_for_sentiment(sentiment)\n",
        "word_5_prob = calculate_word_probability_for_sentiment(len(vocab), train_unigrams, sentiment)\n",
        "\n",
        "print(train_prob_5, '\\n')\n",
        "print(word_5_prob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWebFIkIIrPO",
        "outputId": "5cdd47b1-8a84-4a85-ce57-b1d5fda519f4"
      },
      "id": "fWebFIkIIrPO",
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.12471776 0.25860008 0.19577633 0.27068668 0.15021915] \n",
            "\n",
            "[[0.00000000e+00 0.00000000e+00 1.30434592e-05 0.00000000e+00\n",
            "  0.00000000e+00]\n",
            " [0.00000000e+00 1.05765581e-05 0.00000000e+00 9.98110065e-06\n",
            "  0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 1.30434592e-05 0.00000000e+00\n",
            "  0.00000000e+00]\n",
            " ...\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 9.98110065e-06\n",
            "  0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.58196702e-05\n",
            "  0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 9.98110065e-06\n",
            "  0.00000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting sentiment"
      ],
      "metadata": {
        "id": "fkqmJq9PPyyh"
      },
      "id": "fkqmJq9PPyyh"
    },
    {
      "cell_type": "code",
      "source": [
        "# define function for predicting the sentiment\n",
        "def predict_sentiment(prior_prob, word_prob, data_vec):\n",
        "  return np.argmax(data_vec.dot(prior_prob * word_prob), axis=1)"
      ],
      "metadata": {
        "id": "o-b1fQleJTXp"
      },
      "id": "o-b1fQleJTXp",
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting sentiment for 3-value and 5-value scale and scores associated with it. "
      ],
      "metadata": {
        "id": "I-7r_1G9P2nH"
      },
      "id": "I-7r_1G9P2nH"
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-value sentiment scale for dev and confusion matrix for the predicted and actual dev sentimental 3 value scale\n",
        "dev_pred_sentiment_3 = predict_sentiment(train_prob_3, word_3_prob, dev_vec)\n",
        "confusion_matrix(dev_sentiments_3, dev_pred_sentiment_3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4d4hAdjJiPu",
        "outputId": "a1ca0d4c-06c1-4dbb-cb17-2dec8b9ea865"
      },
      "id": "O4d4hAdjJiPu",
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[263,   0, 123],\n",
              "       [ 72,   1, 108],\n",
              "       [ 64,   0, 369]])"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics\n",
        "print('Accuracy:', accuracy_score(dev_sentiments_3, dev_pred_sentiment_3))\n",
        "print('Precision:', precision_score(dev_sentiments_3, dev_pred_sentiment_3, average='macro'))\n",
        "print('Recall:', recall_score(dev_sentiments_3, dev_pred_sentiment_3, average='macro'))\n",
        "print('F1-score:', f1_score(dev_sentiments_3, dev_pred_sentiment_3, average='macro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R15FHCKxKali",
        "outputId": "2f69f7e3-8fa6-4edf-c7ff-471aba11826b"
      },
      "id": "R15FHCKxKali",
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.633\n",
            "Precision: 0.7580492898913951\n",
            "Recall: 0.5130220025061942\n",
            "F1-score: 0.4651589043336535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-value sentiment scale for train and confusion matrix for the predicted and actual train sentimental 3 value scale\n",
        "train_pred_sentiment_3 = predict_sentiment(train_prob_3, word_3_prob, train_vec)\n",
        "confusion_matrix(train_sentiments_3, train_pred_sentiment_3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2m_CSLNbgs1",
        "outputId": "5061435a-993b-423c-c222-6f3268d7cf66"
      },
      "id": "y2m_CSLNbgs1",
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2514,    1,  371],\n",
              "       [ 593,   32,  849],\n",
              "       [ 139,    0, 3030]])"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics\n",
        "print('Accuracy:', accuracy_score(train_sentiments_3, train_pred_sentiment_3))\n",
        "print('Precision:', precision_score(train_sentiments_3, train_pred_sentiment_3, average='macro'))\n",
        "print('Recall:', recall_score(train_sentiments_3, train_pred_sentiment_3, average='macro'))\n",
        "print('F1-score:', f1_score(train_sentiments_3, train_pred_sentiment_3, average='macro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsPp9MUrb0bW",
        "outputId": "5b3e2bc0-9ee5-4184-d187-a06c319b4548"
      },
      "id": "FsPp9MUrb0bW",
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7406030017266569\n",
            "Precision: 0.8190432760792662\n",
            "Recall: 0.6163163625285016\n",
            "F1-score: 0.5597503385205003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5-value sentiment scale for train and confusion matrix for the predicted and actual train sentimental 5 value scale\n",
        "train_pred_sentiment_5 = predict_sentiment(train_prob_5, word_5_prob, train_vec)\n",
        "confusion_matrix(sentiment, train_pred_sentiment_5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu0_jXv-b43v",
        "outputId": "f219db43-4c28-4449-9ede-74c5e42b7c93"
      },
      "id": "gu0_jXv-b43v",
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  21,  699,    3,  216,    0],\n",
              "       [   4, 1767,    2,  173,    1],\n",
              "       [   7,  595,  215,  655,    2],\n",
              "       [   2,  124,    0, 1911,    1],\n",
              "       [   0,  157,    3,  950,   21]])"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics\n",
        "print('Accuracy:', accuracy_score(sentiment, train_pred_sentiment_5))\n",
        "print('Precision:', precision_score(sentiment, train_pred_sentiment_5, average='macro'))\n",
        "print('Recall:', recall_score(sentiment, train_pred_sentiment_5, average='macro'))\n",
        "print('F1-score:', f1_score(sentiment, train_pred_sentiment_5, average='macro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMYEhDwxcFI7",
        "outputId": "4d19d72c-0dba-4f98-8d81-ee7d5fe58a3c"
      },
      "id": "gMYEhDwxcFI7",
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.52264576969053\n",
            "Precision: 0.6879741065553049\n",
            "Recall: 0.40640550771243833\n",
            "F1-score: 0.32883495209517594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predicting 5-value sentimental scale \n",
        "dev_pred_sentiment_5 = predict_sentiment(train_prob_5, word_5_prob, dev_vec)\n",
        "confusion_matrix(dev_sentiment, dev_pred_sentiment_5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gh_MHFi9Kny3",
        "outputId": "aa22606f-1334-431b-bc90-ffd3f7cc7720"
      },
      "id": "Gh_MHFi9Kny3",
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,  97,   1,  35,   0],\n",
              "       [  2, 173,   3,  75,   0],\n",
              "       [  1,  83,   4,  93,   0],\n",
              "       [  4,  66,   1, 212,   0],\n",
              "       [  0,  24,   0, 125,   1]])"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics\n",
        "print('Accuracy:', accuracy_score(dev_sentiment, dev_pred_sentiment_5))\n",
        "print('Precision:', precision_score(dev_sentiment, dev_pred_sentiment_5, average='macro'))\n",
        "print('Recall:', recall_score(dev_sentiment, dev_pred_sentiment_5, average='macro'))\n",
        "print('F1-score:', f1_score(dev_sentiment, dev_pred_sentiment_5, average='macro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQh4alcyLlbs",
        "outputId": "24e6071e-061e-4abf-8918-d25b89b8f63f"
      },
      "id": "yQh4alcyLlbs",
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.39\n",
            "Precision: 0.44551124487919064\n",
            "Recall: 0.2923354376714985\n",
            "F1-score: 0.2135330136821063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting Test set"
      ],
      "metadata": {
        "id": "SJ_WnFgrQNU6"
      },
      "id": "SJ_WnFgrQNU6"
    },
    {
      "cell_type": "code",
      "source": [
        "# predicting test sentiment of 3-value scale\n",
        "test_pred_sentiments_3 = predict_sentiment(train_prob_3, word_3_prob, test_vec)\n",
        "test_pred_sentiments_3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBdAuuu0MJT_",
        "outputId": "ef9b1f98-8e88-42e3-fee6-54e4a0c9591e"
      },
      "id": "eBdAuuu0MJT_",
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 2, 2, ..., 2, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predicitng test sentiment of 5-value scale\n",
        "test_pred_sentiments_5 = predict_sentiment(train_prob_5, word_5_prob, test_vec)\n",
        "test_pred_sentiments_5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1c3lzOXMVW1",
        "outputId": "1d805d31-5a0c-4629-a5ac-919c1dd699a7"
      },
      "id": "i1c3lzOXMVW1",
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 3, 3, ..., 3, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}